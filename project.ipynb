{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:24:26.095056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 14:24:32.664745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, resnet18, squeezenet1_1\n",
    "from torchvision.models.resnet import ResNet50_Weights, ResNet18_Weights\n",
    "from torchvision.models import efficientnet_v2_l\n",
    "from torchvision.models.efficientnet import EfficientNet_V2_L_Weights\n",
    "from torchvision.models import vgg16, vgg19\n",
    "from torchvision.models.vgg import VGG16_Weights, VGG19_Weights\n",
    "from torchvision.models.squeezenet import SqueezeNet1_1_Weights\n",
    "\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tools"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T14:24:41.020268277Z",
     "start_time": "2023-05-24T14:23:49.327064834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Vgg_face_dag(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Vgg_face_dag, self).__init__()\n",
    "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
    "                     'std': [1, 1, 1],\n",
    "                     'imageSize': [224, 224, 3]}\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
    "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x1 = self.conv1_1(x0)\n",
    "        x2 = self.relu1_1(x1)\n",
    "        x3 = self.conv1_2(x2)\n",
    "        x4 = self.relu1_2(x3)\n",
    "        x5 = self.pool1(x4)\n",
    "        x6 = self.conv2_1(x5)\n",
    "        x7 = self.relu2_1(x6)\n",
    "        x8 = self.conv2_2(x7)\n",
    "        x9 = self.relu2_2(x8)\n",
    "        x10 = self.pool2(x9)\n",
    "        x11 = self.conv3_1(x10)\n",
    "        x12 = self.relu3_1(x11)\n",
    "        x13 = self.conv3_2(x12)\n",
    "        x14 = self.relu3_2(x13)\n",
    "        x15 = self.conv3_3(x14)\n",
    "        x16 = self.relu3_3(x15)\n",
    "        x17 = self.pool3(x16)\n",
    "        x18 = self.conv4_1(x17)\n",
    "        x19 = self.relu4_1(x18)\n",
    "        x20 = self.conv4_2(x19)\n",
    "        x21 = self.relu4_2(x20)\n",
    "        x22 = self.conv4_3(x21)\n",
    "        x23 = self.relu4_3(x22)\n",
    "        x24 = self.pool4(x23)\n",
    "        x25 = self.conv5_1(x24)\n",
    "        x26 = self.relu5_1(x25)\n",
    "        x27 = self.conv5_2(x26)\n",
    "        x28 = self.relu5_2(x27)\n",
    "        x29 = self.conv5_3(x28)\n",
    "        x30 = self.relu5_3(x29)\n",
    "        x31_preflatten = self.pool5(x30)\n",
    "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
    "        x32 = self.fc6(x31)\n",
    "        x33 = self.relu6(x32)\n",
    "        x34 = self.dropout6(x33)\n",
    "        x35 = self.fc7(x34)\n",
    "        x36 = self.relu7(x35)\n",
    "        x37 = self.dropout7(x36)\n",
    "        x38 = self.fc8(x37)\n",
    "        return x38\n",
    "\n",
    "def vgg_face_dag(weights_path='/home/disi/Project-IML/vgg_face_dag.pth'):\n",
    "\n",
    "    model = Vgg_face_dag()\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T14:24:59.131483048Z",
     "start_time": "2023-05-24T14:24:59.119871716Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class LaBocciaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaBocciaModel, self).__init__()\n",
    "        #self.pretrained = squeezenet1_1(weights = SqueezeNet1_1_Weights.DEFAULT)\n",
    "        #self.pretrained = resnet50(weights = ResNet50_Weights.DEFAULT).eval()\n",
    "        #self.pretrained = vgg16(weights = VGG16_Weights.DEFAULT).eval()\n",
    "        #self.pretrained = vgg19(weights = VGG19_Weights.DEFAULT).eval()\n",
    "        #self.pretrained = resnet18(weights = ResNet18_Weights.DEFAULT).eval()\n",
    "        self.pretrained = efficientnet_v2_l(weights = EfficientNet_V2_L_Weights.DEFAULT).eval()\n",
    "        self.pretrained.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:39:27.239419822Z",
     "start_time": "2023-05-15T13:39:27.238118599Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate =  0.001\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 231, in __getitem__\n    sample = self.transform(sample)\nTypeError: 'tuple' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 63\u001B[0m\n\u001B[1;32m     61\u001B[0m   lr \u001B[38;5;241m=\u001B[39m adjust_learning_rate(optimizer, epoch, \u001B[38;5;241m1.616\u001B[39m)\n\u001B[1;32m     62\u001B[0m   learning_rate\u001B[38;5;241m.\u001B[39mappend(lr)\n\u001B[0;32m---> 63\u001B[0m   \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m   \u001B[38;5;66;03m#test(model, device, test_dataloader)\u001B[39;00m\n\u001B[1;32m     65\u001B[0m stop \u001B[38;5;241m=\u001B[39m timeit\u001B[38;5;241m.\u001B[39mdefault_timer()\n",
      "Cell \u001B[0;32mIn[4], line 22\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, device, train_loader, optimizer, epoch)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(model, device, train_loader, optimizer, epoch):\n\u001B[1;32m     20\u001B[0m   model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 22\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[1;32m     23\u001B[0m             \u001B[38;5;66;03m# send the image, target to the device\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     data, target \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     25\u001B[0m             \u001B[38;5;66;03m# flush out the gradients stored in optimizer\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[0;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/_utils.py:644\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    642\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m--> 644\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mTypeError\u001B[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/disi/.virtualenvs/Project-IML/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 231, in __getitem__\n    sample = self.transform(sample)\nTypeError: 'tuple' object is not callable\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import torch.nn.functional as F\n",
    "\n",
    "t = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Resize((224, 224), antialias=True),\n",
    "            torchvision.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ]),\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=\"/dev/shm/gallery_for_dataset/\",\n",
    "        transform=t)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "losses_1 = []\n",
    "losses_2 = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  model.train()\n",
    "\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # send the image, target to the device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "            # flush out the gradients stored in optimizer\n",
    "    optimizer.zero_grad()\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "    output = model(data)\n",
    "            # calculate the loss (use nll_loss in pytorch)\n",
    "    loss = F.nll_loss(output, target)\n",
    "            # do a backward pass\n",
    "    loss.backward()\n",
    "            # update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 100 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "      epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "      100. * batch_idx / len(train_loader), loss.item()))\n",
    "      losses_1.append(loss.item())\n",
    "      losses_2.append(100. * batch_idx / len(train_loader))\n",
    "\n",
    "model = Vgg_face_dag().to(device)\n",
    "learning_rate = []\n",
    "def adjust_learning_rate(optimizer, iter, each):\n",
    "    # sets the learning rate to the initial LR decayed by 0.1 every 'each' iterations\n",
    "    lr = 0.001 * (0.95 ** (iter // each))\n",
    "    state_dict = optimizer.state_dict()\n",
    "    for param_group in state_dict['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.load_state_dict(state_dict)\n",
    "    print(\"Learning rate = \",lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer =  torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1,10):\n",
    "  lr = adjust_learning_rate(optimizer, epoch, 1.616)\n",
    "  learning_rate.append(lr)\n",
    "  train(model, device, train_dataloader, optimizer, epoch)\n",
    "  #test(model, device, test_dataloader)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T14:33:26.723893585Z",
     "start_time": "2023-05-24T14:32:34.485204048Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets\n",
    "from train_functions import *\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "train_paths = \"/dev/shm/train_dataset/\"\n",
    "test_paths = \"/dev/shm/test_dataset/\"\n",
    "\n",
    "\n",
    "t = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Resize((224, 224), antialias=True),\n",
    "            torchvision.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ]),\n",
    "\n",
    "trainset = datasets.ImageFolder(root=train_paths, transform=t)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "testset = datasets.ImageFolder(root=test_paths, transform=t)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)\n",
    "classes = len(trainset.classes)\n",
    "net = Vgg_face_dag().to(device)\n",
    "scheduler = adjust_lr(50, 0.1)\n",
    "lr = 0.005\n",
    "epochs = 50\n",
    "\n",
    "train(epochs, net, trainloader, testloader, device, scheduler, lr, classes, \"vgg_face_dag\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 15:37:38.488311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 15:37:45.209611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from deepface.commons import functions\n",
    "\n",
    "input_dir = \"/home/disi/Project-IML/query/\"\n",
    "output_dir = \"/home/disi/Project-IML/aligned_query/\"\n",
    "\n",
    "tools.face_alignment(input_dir, output_dir)\n",
    "\n",
    "input_dir = \"/home/disi/Project-IML/gallery/\"\n",
    "output_dir = \"/home/disi/Project-IML/aligned_gallery/\"\n",
    "\n",
    "tools.face_alignment(input_dir, output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T15:54:34.416381388Z",
     "start_time": "2023-05-16T15:37:14.033558655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, root: str):\n",
    "\n",
    "    self.root = root\n",
    "\n",
    "    #self.split_ids = io.loadmat(os.path.join(root, \"setid.mat\"))\n",
    "    #self.labels = io.loadmat(os.path.join(root, \"imagelabels.mat\"))[\"labels\"][0]\n",
    "    self.split_ids = {}\n",
    "    self.image_paths = sorted(glob.glob(os.path.join(self.root, \"jpg\", \"*.jpg\")))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_paths)\n",
    "\n",
    "  def __getitem__(self, idx: int):\n",
    "\n",
    "    label = self.labels[idx]\n",
    "    img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "    return img, label\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# PER LA BOCCIA\n",
    "\n",
    "import pickle\n",
    "\n",
    "data_dir = \"/home/disi/Project-IML/aligned_gallery/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gallery_paths = glob.glob(f\"{data_dir}*\", recursive=True)\n",
    "\n",
    "model = LaBocciaModel()\n",
    "model.to(device)\n",
    "\n",
    "embeddings = tools.embeddings_calc(gallery_paths, model, device, data_dir)\n",
    "\n",
    "\n",
    "# save embeddings to disk\n",
    "with open(\"/home/disi/Project-IML/embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "data_dir = \"/home/disi/Project-IML/aligned_query/\"\n",
    "\n",
    "query_paths = glob.glob(f\"{data_dir}*\", recursive=True)\n",
    "\n",
    "queries = tools.embeddings_calc(query_paths, model, device, data_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:43:16.048400422Z",
     "start_time": "2023-05-15T13:40:45.291959231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## PER VGG FACE DAG\n",
    "\n",
    "data_dir = \"/home/disi/Project-IML/aligned_gallery/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gallery_paths = glob.glob(f\"{data_dir}*\", recursive=True)\n",
    "\n",
    "model = vgg_face_dag()\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "embeddings = tools.embeddings_calc(gallery_paths, model, device, data_dir)\n",
    "\n",
    "\n",
    "data_dir = \"/home/disi/Project-IML/aligned_query/\"\n",
    "\n",
    "query_paths = glob.glob(f\"{data_dir}*\", recursive=True)\n",
    "\n",
    "queries = tools.embeddings_calc(query_paths, model, device, data_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T16:04:20.901553528Z",
     "start_time": "2023-05-16T15:54:34.452947329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw accuracy top 10 euclidian distance: 0.1836694418335502\n",
      "Raw accuracy top 5 euclidian distance: 0.4298850574712644\n",
      "Raw accuracy top 1 euclidian distance: 0.5402298850574713\n",
      "Raw accuracy top 10 cosine similarity: 0.21036188666749175\n",
      "Raw accuracy top 5 cosine similarity: 0.45632183908045976\n",
      "Raw accuracy top 1 cosine similarity: 0.5977011494252874\n"
     ]
    }
   ],
   "source": [
    "def euclidian_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "def extract_id(string):\n",
    "    splitter = string.split('/')\n",
    "    splitter = splitter[-1].split('_')\n",
    "    splitter = splitter[0]\n",
    "\n",
    "    return splitter\n",
    "\n",
    "def number_equal(sort):\n",
    "    query_num = 0\n",
    "\n",
    "    for i in sort:\n",
    "        i_id = extract_id(i[0])\n",
    "        if i_id == query_id:\n",
    "            query_num += 1\n",
    "\n",
    "    return query_num\n",
    "\n",
    "query_num_list_10 = []\n",
    "query_num_list_5 = []\n",
    "query_num_list_1 = []\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    counter = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = euclidian_distance(query_embedding, gallery_embedding)\n",
    "\n",
    "        gallery_id = extract_id(gallery)\n",
    "        if gallery_id in counter:\n",
    "            counter[gallery_id] += 1\n",
    "        else:\n",
    "            counter[gallery_id] = 1\n",
    "\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    #print(f\"Query: {query}\")\n",
    "    #print(sorted_distances[:5])\n",
    "    sorted_distances_10 = sorted_distances[:10]\n",
    "    sorted_distances_5 = sorted_distances[:5]\n",
    "    sorted_distances_1 = sorted_distances[:1]\n",
    "\n",
    "    query_id = extract_id(query)\n",
    "\n",
    "    query_num_10 = number_equal(sorted_distances_10)\n",
    "    query_num_5 = number_equal(sorted_distances_5)\n",
    "    query_num_1 = number_equal(sorted_distances_1)\n",
    "\n",
    "    query_num_list_10.append(query_num_10)\n",
    "    query_num_list_5.append(query_num_5)\n",
    "    query_num_list_1.append(query_num_1)\n",
    "    #print(f'Gallery: {counter[query_id]}, found: {query_num}')\n",
    "\n",
    "print(f'Raw accuracy top 10 euclidian distance: {np.array(query_num_list_10).mean()/np.array(list(counter.values())).mean()}')\n",
    "print(f'Raw accuracy top 5 euclidian distance: {np.array(query_num_list_5).mean()/min(np.array(list(counter.values())).mean(),5)}')\n",
    "print(f'Raw accuracy top 1 euclidian distance: {np.array(query_num_list_1).mean()/min(np.array(list(counter.values())).mean(),1)}')\n",
    "\n",
    "# now again but with another distance metric\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "query_num_list_10 = []\n",
    "query_num_5 = []\n",
    "query_num_1 = []\n",
    "for query in queries:\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = cosine_similarity(query_embedding, gallery_embedding)[0][0]\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
    "    #print(f\"Query: {query}\")\n",
    "    #print(sorted_distances[:5])\n",
    "\n",
    "    sorted_distances_10 = sorted_distances[:10]\n",
    "    sorted_distances_5 = sorted_distances[:5]\n",
    "    sorted_distances_1 = sorted_distances[:1]\n",
    "\n",
    "    query_id = extract_id(query)\n",
    "\n",
    "    query_num_10 = number_equal(sorted_distances_10)\n",
    "    query_num_5 = number_equal(sorted_distances_5)\n",
    "    query_num_1 = number_equal(sorted_distances_1)\n",
    "\n",
    "    query_num_list_10.append(query_num_10)\n",
    "    query_num_list_5.append(query_num_5)\n",
    "    query_num_list_1.append(query_num_1)\n",
    "    #print(f'Gallery: {counter[query_id]}, found: {query_num}')\n",
    "\n",
    "print(f'Raw accuracy top 10 cosine similarity: {np.array(query_num_list_10).mean()/np.array(list(counter.values())).mean()}')\n",
    "print(f'Raw accuracy top 5 cosine similarity: {np.array(query_num_list_5).mean()/min(np.array(list(counter.values())).mean(),5)}')\n",
    "print(f'Raw accuracy top 1 cosine similarity: {np.array(query_num_list_1).mean()/min(np.array(list(counter.values())).mean(),1)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T16:09:50.701199007Z",
     "start_time": "2023-05-16T16:04:20.941595220Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "VGGFace pretrained:\n",
    "Raw accuracy top 10 euclidian distance: 0.7358490566037736\n",
    "Raw accuracy top 5 euclidian distance: 0.7\n",
    "Raw accuracy top 1 euclidian distance: 1.0\n",
    "Raw accuracy top 10 cosine similarity: 0.7735849056603773\n",
    "Raw accuracy top 5 cosine similarity: 0.74\n",
    "Raw accuracy top 1 cosine similarity: 1.0\n",
    "\n",
    "\n",
    "EfficientNet:\n",
    "\n",
    "Raw accuracy top 10 euclidian distance: 0.5943396226415094\n",
    "Raw accuracy top 5 euclidian distance: 0.51\n",
    "Raw accuracy top 1 euclidian distance: 1.0\n",
    "Raw accuracy top 10 cosine similarity: 0.6603773584905661\n",
    "Raw accuracy top 5 cosine similarity: 0.52\n",
    "Raw accuracy top 1 cosine similarity: 1.0\n",
    "\n",
    "VGG19\n",
    "\n",
    "Raw accuracy top 10 euclidian distance: 0.37735849056603776\n",
    "Raw accuracy top 5 euclidian distance: 0.26\n",
    "Raw accuracy top 1 euclidian distance: 1.0\n",
    "Raw accuracy top 10 cosine similarity: 0.44339622641509435\n",
    "Raw accuracy top 5 cosine similarity: 0.27999999999999997\n",
    "Raw accuracy top 1 cosine similarity: 1.0\n",
    "\n",
    "Resnet50\n",
    "\n",
    "Raw accuracy top 10 euclidian distance: 0.4716981132075472\n",
    "Raw accuracy top 5 euclidian distance: 0.35\n",
    "Raw accuracy top 1 euclidian distance: 1.0\n",
    "Raw accuracy top 10 cosine similarity: 0.4905660377358491\n",
    "Raw accuracy top 5 cosine similarity: 0.37\n",
    "Raw accuracy top 1 cosine similarity: 1.0\n",
    "\n",
    "VGG16\n",
    "\n",
    "Raw accuracy top 10 euclidian distance: 0.38679245283018865\n",
    "Raw accuracy top 5 euclidian distance: 0.29\n",
    "Raw accuracy top 1 euclidian distance: 1.0\n",
    "Raw accuracy top 10 cosine similarity: 0.37735849056603776\n",
    "Raw accuracy top 5 cosine similarity: 0.29500000000000004\n",
    "Raw accuracy top 1 cosine similarity: 1.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# iterate over the queries\n",
    "# set the number of retrieved images to display\n",
    "num_retrievals = 5\n",
    "\n",
    "# iterate over the queries\n",
    "for query in queries:\n",
    "    # get the query embedding and calculate distances\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = cosine_similarity(query_embedding, gallery_embedding)[0][0]\n",
    "\n",
    "    # sort the distances in descending order\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get the top retrieved image paths and distances\n",
    "    top_paths = [x[0] for x in sorted_distances[:num_retrievals]]\n",
    "    top_similarities = [x[1]*100 for x in sorted_distances[:num_retrievals]]\n",
    "\n",
    "    # load the query image and the top retrieved images\n",
    "    images = [Image.open(query)] + [Image.open(path) for path in top_paths]\n",
    "\n",
    "    # create a figure with subplots for each image\n",
    "    fig, axes = plt.subplots(1, num_retrievals+1, figsize=(15, 5))\n",
    "\n",
    "    # display the query image\n",
    "    axes[0].imshow(np.array(images[0]))\n",
    "    axes[0].set_title(\"Query\")\n",
    "\n",
    "    # display the retrieved images and their distances\n",
    "    for i in range(num_retrievals):\n",
    "        axes[i+1].imshow(np.array(images[i+1]))\n",
    "        axes[i+1].set_title(f\"Similarity: {top_similarities[i]:.2f}%\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# set the number of retrieved images to display\n",
    "\n",
    "# iterate over the queries\n",
    "for query in queries:\n",
    "    # get the query embedding and calculate distances\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = euclidian_distance(query_embedding, gallery_embedding)\n",
    "\n",
    "    # sort the distances in ascending order\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # get the top retrieved image paths and distances\n",
    "    top_paths = [x[0] for x in sorted_distances[:num_retrievals]]\n",
    "    top_distances = [x[1] for x in sorted_distances[:num_retrievals]]\n",
    "\n",
    "    # load the query image and the top retrieved images\n",
    "    images = [Image.open(query)] + [Image.open(path) for path in top_paths]\n",
    "\n",
    "    # create a figure with subplots for each image\n",
    "    fig, axes = plt.subplots(1, num_retrievals+1, figsize=(15, 5))\n",
    "\n",
    "    # display the query image\n",
    "    axes[0].imshow(np.array(images[0]))\n",
    "    axes[0].set_title(\"Query\")\n",
    "\n",
    "    # display the retrieved images and their similarities\n",
    "    for i in range(num_retrievals):\n",
    "        axes[i+1].imshow(np.array(images[i+1]))\n",
    "        axes[i+1].set_title(f\"EuclidianD: {top_distances[i]:.2f}%\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cosine_similarity function returns a matrix of pairwise cosine similarities between the query embedding and the gallery embeddings. Since we are comparing one query image with multiple gallery images, we need to take the first element of the first dimension to get a single similarity score. Also, note that we pass reverse=True to sorted function to sort the distances in descending order since cosine similarity values range between -1 and 1, with higher values indicating more similarity."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
