{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, resnet18, squeezenet1_1\n",
    "from torchvision.models.resnet import ResNet50_Weights, ResNet18_Weights\n",
    "from torchvision.models import efficientnet_v2_l\n",
    "from torchvision.models.efficientnet import EfficientNet_V2_L_Weights\n",
    "from torchvision.models import vgg16, vgg19\n",
    "from torchvision.models.vgg import VGG16_Weights, VGG19_Weights\n",
    "from torchvision.models.squeezenet import SqueezeNet1_1_Weights\n",
    "from torchvision.models import alexnet\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DEFmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DEFmodel, self).__init__()\n",
    "        #self.pretrained = squeezenet1_1(weights = SqueezeNet1_1_Weights.DEFAULT)\n",
    "        #self.pretrained = resnet50(weights = ResNet50_Weights.DEFAULT).eval()\n",
    "        #self.pretrained = vgg16(weights = VGG16_Weights.DEFAULT)\n",
    "        #self.pretrained = vgg19(weights = VGG19_Weights.DEFAULT)\n",
    "        #self.pretrained = resnet18(weights = ResNet18_Weights.DEFAULT).eval()\n",
    "        self.pretrained = efficientnet_v2_l(weights = EfficientNet_V2_L_Weights.DEFAULT).eval()\n",
    "        self.pretrained.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from deepface.commons import functions\n",
    "\n",
    "input_dir = \"/home/disi/Project-IML/query/\"\n",
    "output_dir = \"/home/disi/Project-IML/aligned_query/\"\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    img = cv2.imread(os.path.join(input_dir, file_name))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n",
    "    detection = functions.extract_faces(img = img, enforce_detection=False)\n",
    "    x, y, w, h = detection[0][1].values()\n",
    "    aligned_img = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "    aligned_img = cv2.cvtColor(aligned_img, cv2.COLOR_BGRA2RGB)\n",
    "    aligned_img = cv2.resize(aligned_img, (160, 160))\n",
    "    cv2.imwrite(os.path.join(output_dir, file_name), aligned_img)\n",
    "\n",
    "input_dir = \"/home/disi/Project-IML/gallery/\"\n",
    "output_dir = \"/home/disi/Project-IML/aligned_gallery/\"\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    img = cv2.imread(os.path.join(input_dir, file_name))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n",
    "    detection = functions.extract_faces(img = img, enforce_detection=False)\n",
    "    x, y, w, h = detection[0][1].values()\n",
    "    aligned_img = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "    aligned_img = cv2.cvtColor(aligned_img, cv2.COLOR_BGRA2RGB)\n",
    "    aligned_img = cv2.resize(aligned_img, (160, 160))\n",
    "    cv2.imwrite(os.path.join(output_dir, file_name), aligned_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_dir = \"/home/disi/Project-IML/aligned_gallery/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gallery_paths = glob.glob(f\"{data_dir}*\", recursive=True)\n",
    "\n",
    "model = DEFmodel()\n",
    "model.to(device)\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "for path in gallery_paths:\n",
    "    image = cv2.imread(os.path.join(data_dir, path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    #image = cv2.normalize(np.asarray(image), None, 0, 1.0, cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "    #image = Image.fromarray(image.astype(np.uint8))\n",
    "    image = torchvision.transforms.ToTensor()(image)\n",
    "    image =  torchvision.transforms.Resize((160, 160))(image)\n",
    "    image = torchvision.transforms.Normalize(mean=(0.5, 0.5,0.5), std=(0.5, 0.5, 0.5))(image)\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    embeddings[path] = model(image).detach().cpu().numpy()\n",
    "\n",
    "# save embeddings to disk\n",
    "with open(\"/home/disi/Project-IML/embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "data_dir = \"/home/disi/Project-IML/aligned_query/\"\n",
    "queries = {}\n",
    "query_paths = glob.glob(f\"{data_dir}*\", recursive=True)\n",
    "for query in query_paths:\n",
    "    image = cv2.imread(os.path.join(data_dir, query))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "    #image = cv2.normalize(np.asarray(image), None, 0, 1.0, cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "    #image = Image.fromarray(image.astype(np.uint8))\n",
    "    image = torchvision.transforms.ToTensor()(image)\n",
    "    image =  torchvision.transforms.Resize((160, 160))(image)\n",
    "    image = torchvision.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    queries[query] = model(image).detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def euclidian_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = euclidian_distance(query_embedding, gallery_embedding)\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "    print(f\"Query: {query}\")\n",
    "    print(sorted_distances[:5])\n",
    "\n",
    "# now again but with another distance metric\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = cosine_similarity(query_embedding, gallery_embedding)[0][0]\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(sorted_distances[:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# iterate over the queries\n",
    "# set the number of retrieved images to display\n",
    "num_retrievals = 5\n",
    "\n",
    "# iterate over the queries\n",
    "for query in queries:\n",
    "    # get the query embedding and calculate distances\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = cosine_similarity(query_embedding, gallery_embedding)[0][0]\n",
    "\n",
    "    # sort the distances in descending order\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get the top retrieved image paths and distances\n",
    "    top_paths = [x[0] for x in sorted_distances[:num_retrievals]]\n",
    "    top_similarities = [x[1]*100 for x in sorted_distances[:num_retrievals]]\n",
    "\n",
    "    # load the query image and the top retrieved images\n",
    "    images = [Image.open(query)] + [Image.open(path) for path in top_paths]\n",
    "\n",
    "    # create a figure with subplots for each image\n",
    "    fig, axes = plt.subplots(1, num_retrievals+1, figsize=(15, 5))\n",
    "\n",
    "    # display the query image\n",
    "    axes[0].imshow(np.array(images[0]))\n",
    "    axes[0].set_title(\"Query\")\n",
    "\n",
    "    # display the retrieved images and their distances\n",
    "    for i in range(num_retrievals):\n",
    "        axes[i+1].imshow(np.array(images[i+1]))\n",
    "        axes[i+1].set_title(f\"Similarity: {top_similarities[i]:.2f}%\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# set the number of retrieved images to display\n",
    "\n",
    "# iterate over the queries\n",
    "for query in queries:\n",
    "    # get the query embedding and calculate distances\n",
    "    query_embedding = queries[query]\n",
    "    distances = {}\n",
    "    for gallery in embeddings:\n",
    "        gallery_embedding = embeddings[gallery]\n",
    "        distances[gallery] = euclidian_distance(query_embedding, gallery_embedding)\n",
    "\n",
    "    # sort the distances in ascending order\n",
    "    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # get the top retrieved image paths and distances\n",
    "    top_paths = [x[0] for x in sorted_distances[:num_retrievals]]\n",
    "    top_distances = [x[1] for x in sorted_distances[:num_retrievals]]\n",
    "\n",
    "    # load the query image and the top retrieved images\n",
    "    images = [Image.open(query)] + [Image.open(path) for path in top_paths]\n",
    "\n",
    "    # create a figure with subplots for each image\n",
    "    fig, axes = plt.subplots(1, num_retrievals+1, figsize=(15, 5))\n",
    "\n",
    "    # display the query image\n",
    "    axes[0].imshow(np.array(images[0]))\n",
    "    axes[0].set_title(\"Query\")\n",
    "\n",
    "    # display the retrieved images and their similarities\n",
    "    for i in range(num_retrievals):\n",
    "        axes[i+1].imshow(np.array(images[i+1]))\n",
    "        axes[i+1].set_title(f\"EuclidianD: {top_distances[i]:.2f}%\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cosine_similarity function returns a matrix of pairwise cosine similarities between the query embedding and the gallery embeddings. Since we are comparing one query image with multiple gallery images, we need to take the first element of the first dimension to get a single similarity score. Also, note that we pass reverse=True to sorted function to sort the distances in descending order since cosine similarity values range between -1 and 1, with higher values indicating more similarity."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
